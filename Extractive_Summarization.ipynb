{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extractive Summarization with Discourse Graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3da280ad3f0d4eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this data challenge is to build a extractive summarization system using the discourse graph to classify if each utterance is important to the dialogue transcription. \n",
    "\n",
    "We use a fine-tuned Bert model and the AdaBoost classifier to predict the importance of each utterance. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "666ff05645bb8e8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3990419860f1cbf0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Emile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Emile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 400)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:48:34.954880800Z",
     "start_time": "2023-12-07T14:48:34.715976200Z"
    }
   },
   "id": "241f0878f24815f1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(\"training.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:48:48.999817400Z",
     "start_time": "2023-12-07T14:48:48.978415600Z"
    }
   },
   "id": "44e914e434b48a9d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "training_data_folder = \"training\"\n",
    "annotations_file = \"training_labels.json\"\n",
    "training_csv = \"training.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:48:56.558367500Z",
     "start_time": "2023-12-07T14:48:56.534855800Z"
    }
   },
   "id": "b98a9c97f7550e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tags = ['Acknowledgement',\n",
    " 'Alternation',\n",
    " 'Background',\n",
    " 'Clarification_question',\n",
    " 'Comment',\n",
    " 'Conditional',\n",
    " 'Continuation',\n",
    " 'Contrast',\n",
    " 'Correction',\n",
    " 'Elaboration',\n",
    " 'Explanation',\n",
    " 'Narration',\n",
    " 'Parallel',\n",
    " 'Q-Elab',\n",
    " 'Question-answer_pair',\n",
    " 'Result']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:22:33.143337600Z",
     "start_time": "2023-12-07T15:22:33.085829400Z"
    }
   },
   "id": "733be2374504b99c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the training data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cdc1bd90d849153"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def load_training_data () -> pd.DataFrame:\n",
    "    \"\"\"Loads the training data from the json and txt files\"\"\"\n",
    "    with open(annotations_file, 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "\n",
    "    json_files = [f for f in os.listdir(training_data_folder) if f.endswith('.json')]\n",
    "    json_files.sort()\n",
    "    dfs = []\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(training_data_folder, json_file)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = pd.json_normalize(json.load(file))\n",
    "        shortname = json_file.split(\".\")[0]\n",
    "        data[\"file\"] = shortname\n",
    "        relevance = annotations[shortname]\n",
    "        data[\"relevance\"] = relevance\n",
    "        dfs.append (data)\n",
    "\n",
    "    df = pd.concat (dfs, ignore_index=True)\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "    for tag in tags:\n",
    "        df[tag] = 0\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "    txt_files.sort()\n",
    "    print(\"extraction des donn√©es du graphe (cela va prendre un certain temps)\")\n",
    "    for i,txt_file in tqdm(enumerate(txt_files)):\n",
    "        shortname = txt_file.split(\".\")[0]\n",
    "        with open(os.path.join(training_data_folder,txt_file), 'r') as file:\n",
    "            txt = file.read()\n",
    "        for line in txt.split(\"\\n\"):\n",
    "            if line:\n",
    "                items = line.split(\" \")\n",
    "                tag = items[1]\n",
    "                referenced = items[2]\n",
    "                df.loc[(df['index'] == int(referenced)) & (df['file'] == shortname), tag] = 1\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:33:07.363187500Z",
     "start_time": "2023-12-07T15:33:07.300088900Z"
    }
   },
   "id": "df411f70ef476224"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d4136fda1914b9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def filter_special_characters (text):\n",
    "    \"\"\"Removes special characters from the text\"\"\"\n",
    "    regex = r'[^a-zA-Z0-9\\s.]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "\n",
    "def keep_only_noun_and_verbs (text):\n",
    "    \"\"\"Keeps only nouns and verbs from the text\"\"\"\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "            pos_tagged_noun_verb.append(word)\n",
    "    return \" \".join(pos_tagged_noun_verb)\n",
    "\n",
    "def tokenize_and_filter_stopwords(text):\n",
    "    \"\"\"Tokenize the text\"\"\"\n",
    "    text = filter_special_characters (text)\n",
    "    text = keep_only_noun_and_verbs(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "def sentencize (text):\n",
    "    \"\"\"Splits a text into sentences\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [tokenize_and_filter_stopwords(sentence) for sentence in sentences]\n",
    "    indexes = range(0,len(sentences))\n",
    "    return list(zip(sentences,indexes)), list(zip(tokenized_sentences,indexes))\n",
    "\n",
    "def get_files (df : pd.DataFrame):\n",
    "    return list(set(df[\"file\"].values.tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:33:08.668605700Z",
     "start_time": "2023-12-07T15:33:08.605659200Z"
    }
   },
   "id": "ad05f735a1d29acf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a5ca244b11b49a"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def frequency (token, tokens):\n",
    "    \"\"\"Computes the frequency of a token in a list of tokens\"\"\"\n",
    "    return len([t for t in tokens if t==token])/len(tokens)\n",
    "\n",
    "def inverse_document_frequency (token, tokenized_sentences):\n",
    "    \"\"\"Computes the inverse document frequency of a token in a list of tokenized sentences\"\"\"\n",
    "    d = len(tokenized_sentences)\n",
    "    presence = len([sentence for sentence in tokenized_sentences if token in sentence[0]])\n",
    "    return d/presence\n",
    "\n",
    "def tfidf (tokenized_sentence, tokenized_sentences):\n",
    "    \"\"\"Computes the tfidf score of a tokenized sentence in a list of tokenized sentences\"\"\"\n",
    "    #tokenized_sentence = tokenize_and_filter_stopwords(sentence)\n",
    "    #tokenized_sentences = [tokenize_and_filter_stopwords(sentence) for sentence in sentences]\n",
    "    words = set (tokenized_sentence)\n",
    "    words_scores = {}\n",
    "    for word in words:\n",
    "        tfidf_ = frequency(word,words)*(np.log(1+inverse_document_frequency(word,tokenized_sentences)))\n",
    "        words_scores[word] = tfidf_\n",
    "    return words_scores\n",
    "\n",
    "\n",
    "def sentences_scores (sentences ):\n",
    "    \"\"\"Computes the tfidf score of each sentence in a list of sentences\"\"\"\n",
    "    sentences_scores = {}\n",
    "    for sentence,index in sentences:\n",
    "        words_scores = tfidf(sentence,sentences)\n",
    "        score = sum([words_scores[word] for word in sentence])\n",
    "        sentences_scores[\" \".join(sentence)] = (score, index)\n",
    "    return dict((sentences_scores.items()))\n",
    "\n",
    "def tfidf_sentence_scores (sentences):\n",
    "    \"\"\"Computes the tfidf score of each sentence in a list of sentences\"\"\"\n",
    "    tokenized_sentences = list(zip([tokenize_and_filter_stopwords(sentence) for sentence in sentences],range(len(sentences))))\n",
    "    sentences_scores_ = sentences_scores (tokenized_sentences)\n",
    "    return sentences_scores_\n",
    "\n",
    "def add_tfidf_scores (df : pd.DataFrame):\n",
    "    files = sorted(get_files(df))\n",
    "    for file in tqdm(files):\n",
    "        sentences = df[df[\"file\"]==file][\"text\"].values.tolist()\n",
    "        sentences_and_scores = tfidf_sentence_scores (sentences)\n",
    "        scores = [0] * len(sentences)\n",
    "        for score,index in sentences_and_scores.values():\n",
    "            scores[index] = score\n",
    "\n",
    "        df.loc[df[\"file\"] == file, \"score\"] = scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:33:10.053830800Z",
     "start_time": "2023-12-07T15:33:09.990866200Z"
    }
   },
   "id": "9707dada0a471f96"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "if os.path.exists(training_csv):\n",
    "    df = pd.read_csv (training_csv)\n",
    "else:\n",
    "    print(f\"G√©n√©ration de {training_csv}\")\n",
    "    df = load_training_data ()\n",
    "    add_tfidf_scores(df)\n",
    "    df.to_csv(training_csv,index=False)\n",
    "\n",
    "original_df = df\n",
    "shuffled_df = original_df.sample(frac=1,random_state=42) # shuffle the dataframe for noise reduction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:31:57.761032900Z",
     "start_time": "2023-12-07T15:31:57.326759100Z"
    }
   },
   "id": "d71ceee52c14ad5e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0 speaker                                               text  \\\n3137         3137      ID              they're still very individual tools .   \n37268       37268      UI  So um we've got a <disfmarker> i in this in th...   \n22494       22494      PM                          um I wouldn't know for in   \n19242       19242      ME                  No , splinters would <disfmarker>   \n14452       14452      PM                                          Um okay ,   \n\n       index     file  relevance  Acknowledgement  Alternation  Background  \\\n3137     920  ES2002d          0                0            0           0   \n37268     79  IS1002c          1                0            0           0   \n22494    203  ES2012d          0                0            0           0   \n19242    234  ES2010c          0                1            0           0   \n14452    936  ES2008d          0                1            0           0   \n\n       Clarification_question  ...  Contrast  Correction  Elaboration  \\\n3137                        0  ...         1           0            0   \n37268                       0  ...         0           0            0   \n22494                       0  ...         0           0            1   \n19242                       0  ...         0           0            0   \n14452                       0  ...         0           0            0   \n\n       Explanation  Narration  Parallel  Q-Elab  Question-answer_pair  Result  \\\n3137             0          0         0       0                     0       0   \n37268            0          0         0       0                     0       0   \n22494            0          0         0       0                     0       0   \n19242            0          0         0       0                     0       0   \n14452            0          0         0       0                     0       0   \n\n          score  \n3137   5.713730  \n37268  4.305849  \n22494  4.194438  \n19242  4.127505  \n14452  3.233698  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>speaker</th>\n      <th>text</th>\n      <th>index</th>\n      <th>file</th>\n      <th>relevance</th>\n      <th>Acknowledgement</th>\n      <th>Alternation</th>\n      <th>Background</th>\n      <th>Clarification_question</th>\n      <th>...</th>\n      <th>Contrast</th>\n      <th>Correction</th>\n      <th>Elaboration</th>\n      <th>Explanation</th>\n      <th>Narration</th>\n      <th>Parallel</th>\n      <th>Q-Elab</th>\n      <th>Question-answer_pair</th>\n      <th>Result</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3137</th>\n      <td>3137</td>\n      <td>ID</td>\n      <td>they're still very individual tools .</td>\n      <td>920</td>\n      <td>ES2002d</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5.713730</td>\n    </tr>\n    <tr>\n      <th>37268</th>\n      <td>37268</td>\n      <td>UI</td>\n      <td>So um we've got a &lt;disfmarker&gt; i in this in th...</td>\n      <td>79</td>\n      <td>IS1002c</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.305849</td>\n    </tr>\n    <tr>\n      <th>22494</th>\n      <td>22494</td>\n      <td>PM</td>\n      <td>um I wouldn't know for in</td>\n      <td>203</td>\n      <td>ES2012d</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.194438</td>\n    </tr>\n    <tr>\n      <th>19242</th>\n      <td>19242</td>\n      <td>ME</td>\n      <td>No , splinters would &lt;disfmarker&gt;</td>\n      <td>234</td>\n      <td>ES2010c</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.127505</td>\n    </tr>\n    <tr>\n      <th>14452</th>\n      <td>14452</td>\n      <td>PM</td>\n      <td>Um okay ,</td>\n      <td>936</td>\n      <td>ES2008d</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.233698</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 23 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Visualization\n",
    "shuffled_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:35:56.127111800Z",
     "start_time": "2023-12-07T15:35:56.048369200Z"
    }
   },
   "id": "8a99814d3f1808a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare training and train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ef267642a696a0f"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, logging\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:05:27.714859400Z",
     "start_time": "2023-12-07T16:05:15.274374700Z"
    }
   },
   "id": "ed1d3deafd07fa77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize and create DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eecbf5f635ead02"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.text[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self, device, class_weights, model_name, max_len, lr, loss_type):\n",
    "        self.device = device\n",
    "        self.class_weights = class_weights\n",
    "        if not os.path.exists (model_name):\n",
    "            model_name = \"bert-base-uncased\"\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "\n",
    "        #self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        #self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.5)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "        print(\"Training...\")\n",
    "        train_dataset = CustomDataset(text=x_train, labels=y_train, tokenizer=self.tokenizer, max_len=self.max_len)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        val_dataset = CustomDataset(text=x_val, labels=y_val, tokenizer=self.tokenizer, max_len=self.max_len)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        macro_f1_loss = MacroF1BinaryLoss(self.device)\n",
    "        if os.path.exists(\"best_fscore.txt\"):\n",
    "            with open(\"best_fscore.txt\",\"r\") as file:\n",
    "                best_f1 = float(file.read ())\n",
    "\n",
    "        else:\n",
    "            best_f1 = 0\n",
    "        new_best = 0\n",
    "        for epoch in range(epochs):\n",
    "            for batch in tqdm(train_loader):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)               \n",
    "               \n",
    "                if self.loss_type==\"custom\":\n",
    "                    loss = macro_f1_loss(outputs.logits, labels) # \n",
    "                    loss.backward()\n",
    "                else:\n",
    "                    weighted_loss = torch.nn.functional.cross_entropy(outputs.logits, labels, weight=self.class_weights)\n",
    "                    weighted_loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            #self.scheduler.step()\n",
    "            print(\"End of training.\")\n",
    "            print(\"Starting validation...\")\n",
    "            \n",
    "            # Validation loop\n",
    "            self.model.eval()\n",
    "            val_losses = []\n",
    "            val_correct = 0\n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader):\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['label'].to(self.device)\n",
    "\n",
    "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    \n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    if self.loss_type==\"custom\":\n",
    "                        loss = macro_f1_loss(outputs.logits, labels)\n",
    "                    else:\n",
    "                        loss = torch.nn.functional.cross_entropy(outputs.logits, labels, weight=self.class_weights)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predictions = [torch.argmax(softmax) for softmax in probabilities]\n",
    "                    all_predictions.extend(predictions)\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_correct += (torch.argmax(logits, dim=1) == labels).sum().item()\n",
    "\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            val_accuracy = val_correct / len(x_val)\n",
    "            all_labels_cpu = np.array(all_labels)\n",
    "            all_predictions_cpu = [pred.cpu().numpy() for pred in all_predictions]\n",
    "            val_f1 = f1_score(all_labels_cpu, all_predictions_cpu, pos_label=1)\n",
    "\n",
    "            if val_f1 >= best_f1:\n",
    "                print(\"Best F1-score\")\n",
    "                print(\"Saving model\")\n",
    "                self.model.save_pretrained('best_tuned_bert_model')\n",
    "\n",
    "                self.model.save_pretrained('current_best_tuned_bert_model')\n",
    "                print(\"Saving F1-score \")\n",
    "                with open('best_fscore.txt', 'w') as file:\n",
    "                    # Write the value of the variable to the file\n",
    "\n",
    "                    file.write(str(val_f1))\n",
    "                best_f1 = val_f1\n",
    "                new_best = val_f1\n",
    "            elif val_f1 >= new_best:\n",
    "                print(\"Current best F1-score\")\n",
    "                print(\"Saving model\")\n",
    "                self.model.save_pretrained('current_best_tuned_bert_model')\n",
    "                print(\"current best : \" + str(val_f1))\n",
    "                new_best = val_f1\n",
    "\n",
    "            print (f'Epoch {epoch + 1}/{epochs}, F1 : {val_f1} Loss: {loss.item()}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')\n",
    "            print(f'And best F1 : {best_f1}')\n",
    "            self.model.save_pretrained('last_tuned_bert_model')\n",
    "            \n",
    "            \n",
    "class BertInference:\n",
    "    def __init__ (self, model_path, max_len, device):\n",
    "        self.device = torch.device(device)\n",
    "        self.max_len = max_len\n",
    "        self.model_path = model_path  # Update with the correct path\n",
    "        self.loaded_model = BertForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.loaded_model.to(self.device)\n",
    "        self.loaded_model.eval()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def infer(self,sentences, return_type=\"labels\"):\n",
    "        labels = []\n",
    "        probabilities = []\n",
    "        for sentence in tqdm(sentences):\n",
    "          encoded_input = self.tokenizer.encode_plus(\n",
    "                  sentence,\n",
    "                  add_special_tokens=True,\n",
    "                  max_length=self.max_len,\n",
    "                  return_token_type_ids=False,\n",
    "                  padding=\"max_length\",\n",
    "                  return_attention_mask=True,\n",
    "                  return_tensors='pt',\n",
    "                  truncation=True,\n",
    "              )\n",
    "          encoded_input = {key: tensor.to(self.device) for key, tensor in encoded_input.items()}\n",
    "          with torch.no_grad():\n",
    "              outputs = self.loaded_model(**encoded_input)\n",
    "              logits = outputs.logits\n",
    "              probabilities_ = torch.softmax(logits, dim=1)\n",
    "              predicted_label = torch.argmax(probabilities_, dim=1).tolist()\n",
    "              probabilities.append (probabilities_[0])\n",
    "              labels.append(predicted_label[0])\n",
    "\n",
    "        return labels if return_type==\"labels\" else probabilities\n",
    "\n",
    "\n",
    "class TFVectorizer:\n",
    "  def __init__(self, max_embedding):\n",
    "    self.max_embedding = max_embedding\n",
    "    self.vectorizer = TfidfVectorizer(max_features=max_embedding)\n",
    "\n",
    "  def train (self, x_train):\n",
    "    self.vectorizer.fit_transform(x_train) \n",
    "    # prevent overfitting by using different training sets for vectorizer and classifier\n",
    "\n",
    "  def infer(self,x_infer):\n",
    "    x_test_vectorized = self.vectorizer.transform(x_infer)\n",
    "    return x_test_vectorized\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:05:27.793613400Z",
     "start_time": "2023-12-07T16:05:27.777865500Z"
    }
   },
   "id": "1ba55a0ab1ab7bd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Loss(es)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b91974f597372a"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class MacroF1BinaryLoss(nn.Module):\n",
    "    \"\"\"Custom loss function for binary classification to maximize F1 score.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        super(MacroF1BinaryLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        # Apply sigmoid to get probabilities\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Convert labels to one-hot encoding\n",
    "        labels = labels.to(self.device)\n",
    "        eye = torch.eye(2).to(self.device)\n",
    "        one_hot_labels = eye[labels]\n",
    "\n",
    "        # Calculate true positive, false positive, and false negative\n",
    "        true_positive = (probabilities * one_hot_labels).sum(dim=0)\n",
    "        false_positive = (probabilities * (1 - one_hot_labels)).sum(dim=0)\n",
    "        false_negative = ((1 - probabilities) * one_hot_labels).sum(dim=0)\n",
    "\n",
    "        # Calculate precision, recall, and F1 score for each class\n",
    "        precision = true_positive / (true_positive + false_positive + 1e-10)\n",
    "        recall = true_positive / (true_positive + false_negative + 1e-10)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "        # Macro-F1 is the average F1 score across all classes\n",
    "        macro_f1 = f1_score.mean()\n",
    "\n",
    "        # Use 1 - Macro-F1 as the loss (since we want to minimize it)\n",
    "        loss = 1 - macro_f1\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:05:27.824828Z",
     "start_time": "2023-12-07T16:05:27.809203500Z"
    }
   },
   "id": "930092f65cb185bf"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def test_model (model_name, test_df, device):\n",
    "    \"\"\"Tests the model on the test set\"\"\"\n",
    "    max_len  = 128\n",
    "    bert_inference = BertInference(model_name, max_len, device)\n",
    "\n",
    "    sentences = test_df[\"text\"].values.tolist()\n",
    "    labels = test_df[\"relevance\"].values.tolist()\n",
    "    predicted = bert_inference.infer(sentences)\n",
    "    return f1_score (labels, predicted, pos_label = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:05:27.877318100Z",
     "start_time": "2023-12-07T16:05:27.840518600Z"
    }
   },
   "id": "c4f0fea374bbafe3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b208fb421fcccb2d"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def split_dataset (dataset, split,testing_size):\n",
    "    \"\"\"Splits the dataset into a training, validation and testing set\"\"\"\n",
    "    training_and_val_size = dataset.shape[0] - testing_size\n",
    "    df = dataset[0:training_and_val_size]\n",
    "    train_df, val_df = train_test_split(df, test_size=split, random_state=42)\n",
    "    test_df = shuffled_df[training_and_val_size:]\n",
    "    return train_df, val_df, test_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:07:09.424307100Z",
     "start_time": "2023-12-07T16:07:09.330128400Z"
    }
   },
   "id": "121d3680618883cc"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training :  52603\n",
      "validation :  10020\n",
      "training + validation :  62623\n",
      "test :  10000\n"
     ]
    }
   ],
   "source": [
    "testing_size = 10000\n",
    "split = 0.16\n",
    "split_text = \"016\"\n",
    "\n",
    "train_df, val_df, test_df = split_dataset (shuffled_df, split,testing_size)\n",
    "\n",
    "print(\"training : \", train_df.shape[0])\n",
    "print(\"validation : \", val_df.shape[0])\n",
    "print(\"training + validation : \", train_df.shape[0]+val_df.shape[0])\n",
    "print(\"test : \", test_df.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:07:10.071250300Z",
     "start_time": "2023-12-07T16:07:09.914653400Z"
    }
   },
   "id": "3d8ba1fd60fcd124"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking if the dataset is balanced"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef2708e8b5b42e7"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 proportion in the dataset. Regenerate it if there is an imbalance\n",
      "0.18302741555705493\n",
      "0.18324049959127806\n",
      "0.17904191616766468\n",
      "0.1859\n"
     ]
    }
   ],
   "source": [
    "def proportions (df__):\n",
    "    return df__[df__[\"relevance\"]==1].shape[0]/df__.shape[0]\n",
    "\n",
    "\n",
    "print(\"Class 1 proportion in the dataset. Regenerate it if there is an imbalance\")\n",
    "print(proportions(shuffled_df))\n",
    "print(proportions(train_df))\n",
    "print(proportions(val_df))\n",
    "print(proportions(test_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:06:43.379850200Z",
     "start_time": "2023-12-07T16:06:43.301073300Z"
    }
   },
   "id": "b5088e32853af015"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Bert model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ffe95a23222e5c3"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "assert torch.cuda.is_available()\n",
    "device = \"cuda\"\n",
    "class_weights = torch.tensor([1, 4], dtype=torch.float32).to(device)\n",
    "max_len = 128"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:13:33.734155300Z",
     "start_time": "2023-12-07T16:13:32.043980400Z"
    }
   },
   "id": "79730adada861f88"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate : 2e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4ad84aad23c4057af9e23dbeb906761"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m7\u001B[39m) :\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLearning rate : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m     bert_trainer \u001B[38;5;241m=\u001B[39m \u001B[43mBertTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mclassic\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     bert_trainer\u001B[38;5;241m.\u001B[39mtrain(train_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist(),\n\u001B[0;32m     17\u001B[0m                        train_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevance\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist(),\n\u001B[0;32m     18\u001B[0m                        val_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist(),\n\u001B[0;32m     19\u001B[0m                        val_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevance\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist(),\n\u001B[0;32m     20\u001B[0m                        epochs, batch_size)\n\u001B[0;32m     22\u001B[0m     lr \u001B[38;5;241m=\u001B[39m lr\u001B[38;5;241m*\u001B[39mdecay\n",
      "Cell \u001B[1;32mIn[25], line 42\u001B[0m, in \u001B[0;36mBertTrainer.__init__\u001B[1;34m(self, device, class_weights, model_name, max_len, lr, loss_type)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_len \u001B[38;5;241m=\u001B[39m max_len\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m BertTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mBertForSequenceClassification\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\modeling_utils.py:3037\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3022\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   3023\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m   3024\u001B[0m     cached_file_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3025\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcache_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m: cache_dir,\n\u001B[0;32m   3026\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforce_download\u001B[39m\u001B[38;5;124m\"\u001B[39m: force_download,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3035\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m: commit_hash,\n\u001B[0;32m   3036\u001B[0m     }\n\u001B[1;32m-> 3037\u001B[0m     resolved_archive_file \u001B[38;5;241m=\u001B[39m cached_file(pretrained_model_name_or_path, filename, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcached_file_kwargs)\n\u001B[0;32m   3039\u001B[0m     \u001B[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001B[39;00m\n\u001B[0;32m   3040\u001B[0m     \u001B[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001B[39;00m\n\u001B[0;32m   3041\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_archive_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m filename \u001B[38;5;241m==\u001B[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001B[0;32m   3042\u001B[0m         \u001B[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\utils\\hub.py:430\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    427\u001B[0m user_agent \u001B[38;5;241m=\u001B[39m http_user_agent(user_agent)\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    429\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m--> 430\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to request access at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and pass a token having permission to this repo either \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    448\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    449\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\huggingface_hub\\file_download.py:1461\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001B[0m\n\u001B[0;32m   1458\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m local_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1459\u001B[0m             _check_disk_space(expected_size, local_dir)\n\u001B[1;32m-> 1461\u001B[0m     \u001B[43mhttp_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1462\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1463\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1466\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1468\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m local_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1471\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStoring \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mblob_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\huggingface_hub\\file_download.py:541\u001B[0m, in \u001B[0;36mhttp_get\u001B[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001B[0m\n\u001B[0;32m    539\u001B[0m new_resume_size \u001B[38;5;241m=\u001B[39m resume_size\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 541\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m r\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39mDOWNLOAD_CHUNK_SIZE):\n\u001B[0;32m    542\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m chunk:  \u001B[38;5;66;03m# filter out keep-alive new chunks\u001B[39;00m\n\u001B[0;32m    543\u001B[0m             progress\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(chunk))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\requests\\models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\urllib3\\response.py:628\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp):\n\u001B[1;32m--> 628\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[0;32m    631\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\urllib3\\response.py:567\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[0;32m    564\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[1;32m--> 567\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    569\u001B[0m         flush_decoder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\urllib3\\response.py:533\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m buffer\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    532\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[1;32m--> 533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\http\\client.py:466\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[1;32m--> 466\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\ssl.py:1307\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1303\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1304\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1305\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1306\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\ssl.py:1163\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1163\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1164\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1165\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING 1/2 batch 0 => 16 with cross entropy loss \n",
    "\n",
    "batch_size=16\n",
    "epochs = 4\n",
    "\n",
    "# model_name = \"best_tuned_bert_model\" #if continuing training\n",
    "model_name = \"bert-base-uncased\" #if starting training from scratch\n",
    "size = train_df.shape[0]\n",
    "\n",
    "lr = 2e-5 # we tried other values but this one gave the best results\n",
    "decay = 0.9\n",
    "for i in range(7) :\n",
    "    print(f\"Learning rate : {lr}\")\n",
    "    bert_trainer = BertTrainer(device, class_weights, model_name, max_len,lr,\"classic\")\n",
    "\n",
    "    bert_trainer.train(train_df[\"text\"].values.tolist(),\n",
    "                       train_df[\"relevance\"].values.tolist(),\n",
    "                       val_df[\"text\"].values.tolist(),\n",
    "                       val_df[\"relevance\"].values.tolist(),\n",
    "                       epochs, batch_size)\n",
    "\n",
    "    lr = lr*decay\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:17:32.439033300Z",
     "start_time": "2023-12-07T16:17:13.413705300Z"
    }
   },
   "id": "f2c4d95bd77a0bf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TRAINING 2/2 : batch 16==>32 with MacroF1 loss\n",
    "\n",
    "batch_size=32\n",
    "epochs = 10\n",
    "max_len = 128\n",
    "model_name = \"best_tuned_bert_model\" #We continue training the model we made at last step\n",
    "size = train_df.shape[0]\n",
    "\n",
    "\n",
    "lr = 2e-3 # best learning rate because losses are not on the same scale\n",
    "print(f\"Learning rate : {lr}\")\n",
    "bert_trainer = BertTrainer(device, class_weights, model_name, max_len,lr,\"custom\")\n",
    "\n",
    "bert_trainer.train(train_df[\"text\"].values.tolist(),\n",
    "                   train_df[\"relevance\"].values.tolist(),\n",
    "                   val_df[\"text\"].values.tolist(),\n",
    "                   val_df[\"relevance\"].values.tolist(),\n",
    "                   epochs, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfb2ac4adc0a6a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing Bert on its own"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4391fa64bb5a2bb"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#If you are using cuda and you just trained the Bert model you might need to run the following line\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#torch.cuda.empty_cache()  \u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# HERE WE TEST THE BERT MODEL ON ITS  OWN\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m test_score \u001B[38;5;241m=\u001B[39m \u001B[43mtest_model\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_tuned_bert_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesting score of best iteration : \u001B[39m\u001B[38;5;124m\"\u001B[39m, test_score)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03mtest_score = test_model (\"pretrained_bert_model\", test_df)\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03mprint(\"Testing score of last round best iteration : \", test_score)\"\"\"\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[27], line 8\u001B[0m, in \u001B[0;36mtest_model\u001B[1;34m(model_name, test_df, device)\u001B[0m\n\u001B[0;32m      6\u001B[0m sentences \u001B[38;5;241m=\u001B[39m test_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m      7\u001B[0m labels \u001B[38;5;241m=\u001B[39m test_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevance\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m----> 8\u001B[0m predicted \u001B[38;5;241m=\u001B[39m \u001B[43mbert_inference\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f1_score (labels, predicted, pos_label \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[25], line 176\u001B[0m, in \u001B[0;36mBertInference.infer\u001B[1;34m(self, sentences, return_type)\u001B[0m\n\u001B[0;32m    174\u001B[0m encoded_input \u001B[38;5;241m=\u001B[39m {key: tensor\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m key, tensor \u001B[38;5;129;01min\u001B[39;00m encoded_input\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 176\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloaded_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mencoded_input)\n\u001B[0;32m    177\u001B[0m     logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits\n\u001B[0;32m    178\u001B[0m     probabilities_ \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1560\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1561\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1562\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1564\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1570\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1571\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1572\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1573\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1574\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1576\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   1578\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1004\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m   1006\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m   1007\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1008\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1011\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m   1012\u001B[0m )\n\u001B[1;32m-> 1013\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1019\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1020\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1024\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1025\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1026\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    596\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    597\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    598\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    604\u001B[0m         output_attentions,\n\u001B[0;32m    605\u001B[0m     )\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 607\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    617\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    486\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    487\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    494\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    496\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 497\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    504\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    506\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    419\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    425\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    426\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 427\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    436\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[0;32m    437\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:286\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    278\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    284\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    285\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 286\u001B[0m     mixed_query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    288\u001B[0m     \u001B[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001B[39;00m\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001B[39;00m\n\u001B[0;32m    290\u001B[0m     \u001B[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001B[39;00m\n\u001B[0;32m    291\u001B[0m     is_cross_attention \u001B[38;5;241m=\u001B[39m encoder_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INF554\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#If you are using cuda and you just trained the Bert model you might need to run the following line\n",
    "#torch.cuda.empty_cache()  \n",
    "\n",
    "# HERE WE TEST THE BERT MODEL ON ITS  OWN\n",
    "test_score = test_model (\"best_tuned_bert_model\", test_df, \"cuda\")\n",
    "print(\"Testing score of best iteration : \", test_score)\n",
    "\"\"\"\n",
    "test_score = test_model (\"pretrained_bert_model\", test_df)\n",
    "print(\"Testing score of last round best iteration : \", test_score)\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:19:59.228668300Z",
     "start_time": "2023-12-07T16:19:48.139616900Z"
    }
   },
   "id": "5c8da3aca87d1ddf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5228b2fda89e90f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
