{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "création de graphe sur un fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using graph files not successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "path_to_training = Path(\"training\")\n",
    "path_to_test = Path(\"test\")\n",
    "\n",
    "#######################################################################\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "test_set = ['ES2003', 'ES2004', 'ES2011', 'ES2014', 'IS1008', 'IS1009', 'TS3003', 'TS3004', 'TS3006', 'TS3007']\n",
    "test_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in test_set])\n",
    "\n",
    "######################################################################\n",
    "names=['Continuation', 'Explanation', 'Elaboration', 'Acknowledgement', 'Comment', 'Result', 'Narration','Question-answer_pair','Contrast','Clarification_question','Background','Alternation','Conditional','Q-Elab','Correction','Parallel']\n",
    "\n",
    "\n",
    "\n",
    "graphes=[]\n",
    "X_training = []\n",
    "all_lens={}\n",
    "for transcription_id in training_set:\n",
    "    X_training_part=[]\n",
    "    l=0\n",
    "    with open(path_to_training / f\"{transcription_id}.json\", \"r\") as file:\n",
    "        transcription = json.load(file)\n",
    "    \n",
    "    for utterance in transcription:\n",
    "        X_training_part.append(utterance[\"speaker\"] + \": \" + utterance[\"text\"])\n",
    "        l+=1\n",
    "    all_lens[transcription_id]  = l  # On connait maintenant la taille du graphe\n",
    "    X_training.extend(X_training_part)\n",
    "    \n",
    "    G=nx.Graph()\n",
    "    \n",
    "    for index, phrase in enumerate(X_training_part):\n",
    "        G.add_node(index, text=phrase)\n",
    "    edges=[]\n",
    "    with open(path_to_training / f\"{transcription_id}.txt\", \"r\") as file:\n",
    "        transcription = file.read().splitlines()\n",
    "        for utterance in transcription:\n",
    "            splitted = utterance.split()\n",
    "            a,b=int(splitted[0]),int(splitted[2])\n",
    "            #c,d=X_training_part[a],X_training_part[b]\n",
    "            edges.append((a,b))\n",
    "    G.add_edges_from(edges)  \n",
    "    graphes.append(G)      \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nfor n, attributs in graphes[0].nodes(data=True):\\n    print(f\"Nœud {n}: {attributs}\") '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "for n, attributs in graphes[0].nodes(data=True):\n",
    "    print(f\"Nœud {n}: {attributs}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n# Supposons que \\'graphes[0]\\' est votre graphe\\nfor edge in graphes[0].edges(data=True):\\n    node1, node2, attributs = edge\\n    print(f\"Arête entre les nœuds {node1} et {node2} : {attributs}\")\\n\\n    poids = attributs.get(\\'poids\\', None)\\n    if poids is not None:\\n        print(f\"Poids de l\\'arête : {poids}\")\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "# Supposons que 'graphes[0]' est votre graphe\n",
    "for edge in graphes[0].edges(data=True):\n",
    "    node1, node2, attributs = edge\n",
    "    print(f\"Arête entre les nœuds {node1} et {node2} : {attributs}\")\n",
    "\n",
    "    poids = attributs.get('poids', None)\n",
    "    if poids is not None:\n",
    "        print(f\"Poids de l'arête : {poids}\")\n",
    "\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphe_fusionne = nx.Graph()\n",
    "for g in graphes:\n",
    "    graphe_fusionne = nx.compose(graphe_fusionne, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = []\n",
    "with open(\"training_labels.json\", \"r\") as file:\n",
    "    training_labels = json.load(file)\n",
    "for transcription_id in training_set:\n",
    "    y_training+=training_labels[transcription_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
