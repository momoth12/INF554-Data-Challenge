{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2568057",
   "metadata": {
    "id": "d2568057"
   },
   "source": [
    "#  Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajouter le nombre des mts dans les features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e807f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb69bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26475e6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26475e6f",
    "outputId": "596b4632-0bff-48c2-cf24-566aa3ec8556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cNk1I5OA0-xr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "cNk1I5OA0-xr",
    "outputId": "468b0e19-90a6-43b1-efa6-0dce3f8072f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.path.exists(\"training.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e66a451d",
   "metadata": {
    "id": "e66a451d"
   },
   "outputs": [],
   "source": [
    "training_data_folder = \"training\"\n",
    "annotations_file = \"training_labels.json\"\n",
    "training_csv = \"training.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3547eaa7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "3547eaa7",
    "outputId": "87d8b94c-72f2-440b-a7bd-215fe28df66b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tags = set()\n",
    "def get_tags_from_files (): #Run only once\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        with open(os.path.join(training_data_folder,txt_file), 'r') as file:\n",
    "            txt = file.read()\n",
    "        for line in txt.split(\"\\n\"):\n",
    "            if line:\n",
    "                tag = line.split(\" \")[1]\n",
    "                if not tag in tags:\n",
    "                    tags.add (tag)\n",
    "#get_tags_from_files()\n",
    "tags = ['Acknowledgement',\n",
    " 'Alternation',\n",
    " 'Background',\n",
    " 'Clarification_question',\n",
    " 'Comment',\n",
    " 'Conditional',\n",
    " 'Continuation',\n",
    " 'Contrast',\n",
    " 'Correction',\n",
    " 'Elaboration',\n",
    " 'Explanation',\n",
    " 'Narration',\n",
    " 'Parallel',\n",
    " 'Q-Elab',\n",
    " 'Question-answer_pair',\n",
    " 'Result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc9c03",
   "metadata": {
    "id": "66cc9c03"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9017c30",
   "metadata": {
    "id": "f9017c30"
   },
   "outputs": [],
   "source": [
    "#TFIDF computing\n",
    "\n",
    "def filter_special_characters (text):\n",
    "    regex = r'[^a-zA-Z0-9\\s.]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "\n",
    "def keep_only_noun_and_verbs (text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "            pos_tagged_noun_verb.append(word)\n",
    "    return \" \".join(pos_tagged_noun_verb)\n",
    "\n",
    "def tokenize_and_filter_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    text = filter_special_characters (text)\n",
    "    text = keep_only_noun_and_verbs(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "def frequency (token, tokens):\n",
    "    return len([t for t in tokens if t==token])/len(tokens)\n",
    "\n",
    "def inverse_document_frequency (token, tokenized_sentences):\n",
    "    d = len(tokenized_sentences)\n",
    "    presence = len([sentence for sentence in tokenized_sentences if token in sentence[0]])\n",
    "    return d/presence\n",
    "\n",
    "def tfidf (tokenized_sentence, tokenized_sentences):\n",
    "    #tokenized_sentence = tokenize_and_filter_stopwords(sentence)\n",
    "    #tokenized_sentences = [tokenize_and_filter_stopwords(sentence) for sentence in sentences]\n",
    "    words = set (tokenized_sentence)\n",
    "    words_scores = {}\n",
    "    for word in words:\n",
    "        tfidf_ = frequency(word,words)*(np.log(1+inverse_document_frequency(word,tokenized_sentences)))\n",
    "        words_scores[word] = tfidf_\n",
    "    return words_scores\n",
    "\n",
    "\n",
    "def sentencize (text):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [tokenize_and_filter_stopwords(sentence) for sentence in sentences]\n",
    "    indexes = range(0,len(sentences))\n",
    "    return list(zip(sentences,indexes)), list(zip(tokenized_sentences,indexes))\n",
    "\n",
    "def sentences_scores (sentences ):\n",
    "    sentences_scores = {}\n",
    "    for sentence,index in sentences:\n",
    "        words_scores = tfidf(sentence,sentences)\n",
    "        score = sum([words_scores[word] for word in sentence])\n",
    "        sentences_scores[\" \".join(sentence)] = (score, index)\n",
    "    return dict((sentences_scores.items()))\n",
    "\n",
    "\n",
    "def extractive_summary (sentences, threshold):\n",
    "\n",
    "    tokenized_sentences = list(zip([tokenize_and_filter_stopwords(sentence) for sentence in sentences],\n",
    "                                   range(len(sentences))))\n",
    "\n",
    "    sentences_scores_ = list(sentences_scores (tokenized_sentences).items())\n",
    "\n",
    "    sentences_indexes = [sentence_and_score[1][1] for sentence_and_score in  sentences_scores_ if sentence_and_score[1][0]>=threshold]\n",
    "    sentences_indexes.sort()\n",
    "    summary = [sentences[index] for index in sentences_indexes]\n",
    "\n",
    "    return summary\n",
    "    #print(summary)\n",
    "\n",
    "\n",
    "def tfidf_sentence_scores (sentences):\n",
    "    tokenized_sentences = list(zip([tokenize_and_filter_stopwords(sentence) for sentence in sentences],range(len(sentences))))\n",
    "    sentences_scores_ = sentences_scores (tokenized_sentences)\n",
    "    return sentences_scores_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be49e3a8",
   "metadata": {
    "id": "be49e3a8"
   },
   "outputs": [],
   "source": [
    "#Loading training data from json and txt files. \n",
    "\n",
    "def load_training_data ():\n",
    "\n",
    "    with open(annotations_file, 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "\n",
    "    json_files = [f for f in os.listdir(training_data_folder) if f.endswith('.json')]\n",
    "    json_files.sort()\n",
    "    dfs = []\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(training_data_folder, json_file)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = pd.json_normalize(json.load(file))\n",
    "        shortname = json_file.split(\".\")[0]\n",
    "        data[\"file\"] = shortname\n",
    "        relevance = annotations[shortname]\n",
    "        data[\"relevance\"] = relevance\n",
    "        dfs.append (data)\n",
    "\n",
    "    df = pd.concat (dfs, ignore_index=True)\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "    for tag in tags:\n",
    "        df[tag] = 0\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "    txt_files.sort()\n",
    "    print(\"extraction des données du graphe (cela va prendre un certain temps)\")\n",
    "    for i,txt_file in tqdm(enumerate(txt_files)):\n",
    "        shortname = txt_file.split(\".\")[0]\n",
    "        with open(os.path.join(training_data_folder,txt_file), 'r') as file:\n",
    "            txt = file.read()\n",
    "        for line in txt.split(\"\\n\"):\n",
    "\n",
    "            if line:\n",
    "                items = line.split(\" \")\n",
    "                tag = items[1]\n",
    "                referenced = items[2]\n",
    "                df.loc[(df['index'] == int(referenced)) & (df['file'] == shortname), tag] = 1\n",
    "    return df\n",
    "\n",
    "def get_files ():\n",
    "    return list(set(df[\"file\"].values.tolist()))\n",
    "\n",
    "def add_tfidf_scores ():\n",
    "    files = sorted(get_files())\n",
    "    for file in tqdm(files):\n",
    "        sentences = df[df[\"file\"]==file][\"text\"].values.tolist()\n",
    "        sentences_and_scores = tfidf_sentence_scores (sentences)\n",
    "        scores = [0] * len(sentences)\n",
    "        for score,index in sentences_and_scores.values():\n",
    "            scores[index] = score\n",
    "\n",
    "        df.loc[df[\"file\"] == file, \"score\"] = scores\n",
    "        \n",
    "        \n",
    "def split_dataset (dataset, split,testing_size):\n",
    "  training_and_val_size = dataset.shape[0] - testing_size\n",
    "  df = dataset[0:training_and_val_size]\n",
    "  train_df, val_df = train_test_split(df, test_size=split, random_state=42)\n",
    "  test_df = shuffled_df[training_and_val_size:]\n",
    "  return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9822f5c7",
   "metadata": {
    "id": "9822f5c7"
   },
   "outputs": [],
   "source": [
    "#You only need to run load_training_data once\n",
    "if os.path.exists(training_csv):\n",
    "    df = pd.read_csv (training_csv)\n",
    "else:\n",
    "    print(f\"Génération de {training_csv}\")\n",
    "    df = load_training_data ()\n",
    "    add_tfidf_scores()\n",
    "    df.to_csv(training_csv,index=False)\n",
    "\n",
    "original_df = df\n",
    "shuffled_df = original_df.sample(frac=1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add97065",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "add97065",
    "outputId": "9fb3458e-3f95-432f-c15b-a42d05a5b516"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "      <th>file</th>\n",
       "      <th>relevance</th>\n",
       "      <th>Acknowledgement</th>\n",
       "      <th>Alternation</th>\n",
       "      <th>Background</th>\n",
       "      <th>Clarification_question</th>\n",
       "      <th>...</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Correction</th>\n",
       "      <th>Elaboration</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Narration</th>\n",
       "      <th>Parallel</th>\n",
       "      <th>Q-Elab</th>\n",
       "      <th>Question-answer_pair</th>\n",
       "      <th>Result</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>Okay</td>\n",
       "      <td>0</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PM</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>2</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.954745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>3</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.531381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PM</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>4</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.949238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 speaker                                               text  \\\n",
       "0           0      PM                                               Okay   \n",
       "1           1      PM                                              Right   \n",
       "2           2      PM  <vocalsound> Um well this is the kick-off meet...   \n",
       "3           3      PM                             Um <vocalsound> and um   \n",
       "4           4      PM  this is just what we're gonna be doing over th...   \n",
       "\n",
       "   index     file  relevance  Acknowledgement  Alternation  Background  \\\n",
       "0      0  ES2002a          0                0            0           0   \n",
       "1      1  ES2002a          0                0            0           0   \n",
       "2      2  ES2002a          1                0            0           0   \n",
       "3      3  ES2002a          0                0            0           0   \n",
       "4      4  ES2002a          0                0            0           0   \n",
       "\n",
       "   Clarification_question  ...  Contrast  Correction  Elaboration  \\\n",
       "0                       0  ...         0           0            0   \n",
       "1                       0  ...         0           0            0   \n",
       "2                       0  ...         0           0            0   \n",
       "3                       0  ...         0           0            0   \n",
       "4                       0  ...         0           0            1   \n",
       "\n",
       "   Explanation  Narration  Parallel  Q-Elab  Question-answer_pair  Result  \\\n",
       "0            0          0         0       0                     0       0   \n",
       "1            0          0         0       0                     0       0   \n",
       "2            0          0         0       0                     0       0   \n",
       "3            1          0         0       0                     0       0   \n",
       "4            0          0         0       0                     0       0   \n",
       "\n",
       "      score  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  3.954745  \n",
       "3  3.531381  \n",
       "4  4.949238  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "993ecbdd",
   "metadata": {
    "id": "993ecbdd"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b43506e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training :  52603\n",
      "validation :  10020\n",
      "training + validation :  62623\n",
      "test :  10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testing_size = 10000\n",
    "split = 0.16 #iftraining bert, else 0.8 for just tfidf and classifier because 0.16 takes to much time to execute\n",
    "split_text = \"016\"\n",
    "\n",
    "train_df, val_df, test_df = split_dataset (shuffled_df, split,testing_size)\n",
    "\n",
    "print(\"training : \", train_df.shape[0])\n",
    "print(\"validation : \", val_df.shape[0])\n",
    "print(\"training + validation : \", train_df.shape[0]+val_df.shape[0])\n",
    "print(\"test : \", test_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cce59279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cce59279",
    "outputId": "61034c35-c074-4c58-c0c4-70ff32acc47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit and transform done\n",
      "test transform done\n",
      "training samples :  10000\n",
      "embedding dimension :  317\n",
      "classifier trained\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.85      8226\n",
      "           1       0.43      0.72      0.54      1794\n",
      "\n",
      "    accuracy                           0.78     10020\n",
      "   macro avg       0.68      0.75      0.69     10020\n",
      "weighted avg       0.84      0.78      0.80     10020\n",
      "\n",
      "ADA\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10000, 10020]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mouha\\Desktop\\3A\\INF554\\DATA CHALLENGE\\DATA\\INF554-Data-Challenge\\tfidf_model_.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mouha/Desktop/3A/INF554/DATA%20CHALLENGE/DATA/INF554-Data-Challenge/tfidf_model_.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m sample_weights \u001b[39m=\u001b[39m compute_sample_weights(y_train, class_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mouha/Desktop/3A/INF554/DATA%20CHALLENGE/DATA/INF554-Data-Challenge/tfidf_model_.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m ada_classifier \u001b[39m=\u001b[39m AdaBoostClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mouha/Desktop/3A/INF554/DATA%20CHALLENGE/DATA/INF554-Data-Challenge/tfidf_model_.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m ada_classifier\u001b[39m.\u001b[39;49mfit(x_train_combined, y_val, sample_weight\u001b[39m=\u001b[39;49msample_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mouha/Desktop/3A/INF554/DATA%20CHALLENGE/DATA/INF554-Data-Challenge/tfidf_model_.ipynb#X21sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m ada_predictions \u001b[39m=\u001b[39m ada_classifier\u001b[39m.\u001b[39mpredict(x_val_combined)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mouha/Desktop/3A/INF554/DATA%20CHALLENGE/DATA/INF554-Data-Challenge/tfidf_model_.ipynb#X21sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClassification Report:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:135\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39m@_fit_context\u001b[39m(\n\u001b[0;32m    111\u001b[0m     \u001b[39m# AdaBoost*.estimator is not validated yet\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a boosted classifier/regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    136\u001b[0m         X,\n\u001b[0;32m    137\u001b[0m         y,\n\u001b[0;32m    138\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    139\u001b[0m         ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    140\u001b[0m         allow_nd\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    141\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    142\u001b[0m         y_numeric\u001b[39m=\u001b[39;49mis_regressor(\u001b[39mself\u001b[39;49m),\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    145\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    146\u001b[0m         sample_weight, X, np\u001b[39m.\u001b[39mfloat64, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m     sample_weight \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m sample_weight\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1166\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10000, 10020]"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_sample_weights(y, class_weights):\n",
    "    sample_weights = [class_weights[label] for label in y]\n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "\n",
    "train_df = train_df [0:10000]\n",
    "\n",
    "additional_features = tags + [\"score\"]\n",
    "max_embedding = 300\n",
    "\n",
    "class_weights = [1,3]\n",
    "df = shuffled_df\n",
    "x_train_sentences = train_df [\"text\"].values.tolist()\n",
    "y_train = train_df[\"relevance\"].values.tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_embedding)\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train_sentences)\n",
    "\n",
    "print(\"fit and transform done\")\n",
    "\n",
    "x_train_combined = np.concatenate([x_train_vectorized.toarray(), train_df[additional_features]], axis=1)\n",
    "print(\"test transform done\")\n",
    "print(\"training samples : \", len(y_train))\n",
    "print(\"embedding dimension : \", x_train_combined.shape[1])\n",
    "\n",
    "\n",
    "classifier = SVC(kernel='linear',class_weight={0:class_weights[0],1:class_weights[1]})\n",
    "classifier.fit(x_train_combined, y_train)\n",
    "\n",
    "print(\"classifier trained\")\n",
    "\n",
    "x_val_sentences = val_df [\"text\"].values.tolist()\n",
    "y_val = val_df[\"relevance\"].values.tolist()\n",
    "\n",
    "x_val_vectorized = vectorizer.transform(x_val_sentences)\n",
    "\n",
    "x_val_combined = np.concatenate([x_val_vectorized.toarray(), val_df[additional_features]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = classifier.predict(x_val_combined)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, predictions))\n",
    "#size = size*2\n",
    "\n",
    "print(\"ADA\")\n",
    "\n",
    "sample_weights = compute_sample_weights(y_train, class_weights)\n",
    "ada_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_classifier.fit(x_train_combined, y_val, sample_weight=sample_weights)\n",
    "\n",
    "ada_predictions = ada_classifier.predict(x_val_combined)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, ada_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c4750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "history_visible": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
