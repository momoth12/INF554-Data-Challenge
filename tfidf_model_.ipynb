{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2568057",
   "metadata": {
    "id": "d2568057"
   },
   "source": [
    "#  Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajouter le nombre des mts dans les features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724da0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/remi/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: nltk in /home/remi/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/remi/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/remi/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/remi/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /home/remi/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: transformers in /home/remi/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/remi/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/remi/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/remi/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/remi/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/remi/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/remi/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/remi/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: scikit-learn in /home/remi/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/remi/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/remi/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/remi/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/remi/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm\n",
    "! pip install nltk\n",
    "! pip install transformers\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e807f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb69bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26475e6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26475e6f",
    "outputId": "596b4632-0bff-48c2-cf24-566aa3ec8556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cNk1I5OA0-xr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "cNk1I5OA0-xr",
    "outputId": "468b0e19-90a6-43b1-efa6-0dce3f8072f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.path.exists(\"training.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66a451d",
   "metadata": {
    "id": "e66a451d"
   },
   "outputs": [],
   "source": [
    "training_data_folder = \"training\"\n",
    "annotations_file = \"training_labels.json\"\n",
    "training_csv = \"training.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3547eaa7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "3547eaa7",
    "outputId": "87d8b94c-72f2-440b-a7bd-215fe28df66b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tags = set()\n",
    "def get_tags_from_files (): #Run only once\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        with open(os.path.join(training_data_folder,txt_file), 'r') as file:\n",
    "            txt = file.read()\n",
    "        for line in txt.split(\"\\n\"):\n",
    "            if line:\n",
    "                tag = line.split(\" \")[1]\n",
    "                if not tag in tags:\n",
    "                    tags.add (tag)\n",
    "#get_tags_from_files()\n",
    "tags = ['Acknowledgement',\n",
    " 'Alternation',\n",
    " 'Background',\n",
    " 'Clarification_question',\n",
    " 'Comment',\n",
    " 'Conditional',\n",
    " 'Continuation',\n",
    " 'Contrast',\n",
    " 'Correction',\n",
    " 'Elaboration',\n",
    " 'Explanation',\n",
    " 'Narration',\n",
    " 'Parallel',\n",
    " 'Q-Elab',\n",
    " 'Question-answer_pair',\n",
    " 'Result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc9c03",
   "metadata": {
    "id": "66cc9c03"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9017c30",
   "metadata": {
    "id": "f9017c30"
   },
   "outputs": [],
   "source": [
    "#TFIDF computing\n",
    "\n",
    "def filter_special_characters (text):\n",
    "    regex = r'[^a-zA-Z0-9\\s.]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "\n",
    "def keep_only_noun_and_verbs (text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "            pos_tagged_noun_verb.append(word)\n",
    "    return \" \".join(pos_tagged_noun_verb)\n",
    "\n",
    "def tokenize_and_filter_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    text = filter_special_characters (text)\n",
    "    text = keep_only_noun_and_verbs(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "def frequency (token, tokens):\n",
    "    return len([t for t in tokens if t==token])/len(tokens)\n",
    "\n",
    "def inverse_document_frequency (token, tokenized_sentences):\n",
    "    d = len(tokenized_sentences)\n",
    "    presence = len([sentence for sentence in tokenized_sentences if token in sentence[0]])\n",
    "    return d/presence\n",
    "\n",
    "def tfidf (tokenized_sentence, tokenized_sentences):\n",
    "    #tokenized_sentence = tokenize_and_filter_stopwords(sentence)\n",
    "    #tokenized_sentences = [tokenize_and_filter_stopwords(sentence) for sentence in sentences]\n",
    "    words = set (tokenized_sentence)\n",
    "    words_scores = {}\n",
    "    for word in words:\n",
    "        tfidf_ = frequency(word,words)*(np.log(1+inverse_document_frequency(word,tokenized_sentences)))\n",
    "        words_scores[word] = tfidf_\n",
    "    return words_scores\n",
    "\n",
    "\n",
    "def sentencize (text):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [tokenize_and_filter_stopwords(sentence) for sentence in sentences]\n",
    "    indexes = range(0,len(sentences))\n",
    "    return list(zip(sentences,indexes)), list(zip(tokenized_sentences,indexes))\n",
    "\n",
    "def sentences_scores (sentences ):\n",
    "    sentences_scores = {}\n",
    "    for sentence,index in sentences:\n",
    "        words_scores = tfidf(sentence,sentences)\n",
    "        score = sum([words_scores[word] for word in sentence])\n",
    "        sentences_scores[\" \".join(sentence)] = (score, index)\n",
    "    return dict((sentences_scores.items()))\n",
    "\n",
    "\n",
    "def extractive_summary (sentences, threshold):\n",
    "\n",
    "    tokenized_sentences = list(zip([tokenize_and_filter_stopwords(sentence) for sentence in sentences],\n",
    "                                   range(len(sentences))))\n",
    "\n",
    "    sentences_scores_ = list(sentences_scores (tokenized_sentences).items())\n",
    "\n",
    "    sentences_indexes = [sentence_and_score[1][1] for sentence_and_score in  sentences_scores_ if sentence_and_score[1][0]>=threshold]\n",
    "    sentences_indexes.sort()\n",
    "    summary = [sentences[index] for index in sentences_indexes]\n",
    "\n",
    "    return summary\n",
    "    #print(summary)\n",
    "\n",
    "\n",
    "def tfidf_sentence_scores (sentences):\n",
    "    tokenized_sentences = list(zip([tokenize_and_filter_stopwords(sentence) for sentence in sentences],range(len(sentences))))\n",
    "    sentences_scores_ = sentences_scores (tokenized_sentences)\n",
    "    return sentences_scores_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be49e3a8",
   "metadata": {
    "id": "be49e3a8"
   },
   "outputs": [],
   "source": [
    "#Loading training data from json and txt files. \n",
    "\n",
    "def load_training_data ():\n",
    "\n",
    "    with open(annotations_file, 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "\n",
    "    json_files = [f for f in os.listdir(training_data_folder) if f.endswith('.json')]\n",
    "    json_files.sort()\n",
    "    dfs = []\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(training_data_folder, json_file)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = pd.json_normalize(json.load(file))\n",
    "        shortname = json_file.split(\".\")[0]\n",
    "        data[\"file\"] = shortname\n",
    "        relevance = annotations[shortname]\n",
    "        data[\"relevance\"] = relevance\n",
    "        dfs.append (data)\n",
    "\n",
    "    df = pd.concat (dfs, ignore_index=True)\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "    for tag in tags:\n",
    "        df[tag] = 0\n",
    "    txt_files = [f for f in os.listdir(training_data_folder) if f.endswith('.txt')]\n",
    "    txt_files.sort()\n",
    "    print(\"extraction des données du graphe (cela va prendre un certain temps)\")\n",
    "    for i,txt_file in tqdm(enumerate(txt_files)):\n",
    "        shortname = txt_file.split(\".\")[0]\n",
    "        with open(os.path.join(training_data_folder,txt_file), 'r') as file:\n",
    "            txt = file.read()\n",
    "        for line in txt.split(\"\\n\"):\n",
    "\n",
    "            if line:\n",
    "                items = line.split(\" \")\n",
    "                tag = items[1]\n",
    "                referenced = items[2]\n",
    "                df.loc[(df['index'] == int(referenced)) & (df['file'] == shortname), tag] = 1\n",
    "    return df\n",
    "\n",
    "def get_files ():\n",
    "    return list(set(df[\"file\"].values.tolist()))\n",
    "\n",
    "def add_tfidf_scores ():\n",
    "    files = sorted(get_files())\n",
    "    for file in tqdm(files):\n",
    "        sentences = df[df[\"file\"]==file][\"text\"].values.tolist()\n",
    "        sentences_and_scores = tfidf_sentence_scores (sentences)\n",
    "        scores = [0] * len(sentences)\n",
    "        for score,index in sentences_and_scores.values():\n",
    "            scores[index] = score\n",
    "\n",
    "        df.loc[df[\"file\"] == file, \"score\"] = scores\n",
    "        \n",
    "        \n",
    "def split_dataset (dataset, split,testing_size):\n",
    "  training_and_val_size = dataset.shape[0] - testing_size\n",
    "  df = dataset[0:training_and_val_size]\n",
    "  train_df, val_df = train_test_split(df, test_size=split, random_state=42)\n",
    "  test_df = shuffled_df[training_and_val_size:]\n",
    "  return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9822f5c7",
   "metadata": {
    "id": "9822f5c7"
   },
   "outputs": [],
   "source": [
    "#You only need to run load_training_data once\n",
    "if os.path.exists(training_csv):\n",
    "    df = pd.read_csv (training_csv)\n",
    "else:\n",
    "    print(f\"Génération de {training_csv}\")\n",
    "    df = load_training_data ()\n",
    "    add_tfidf_scores()\n",
    "    df.to_csv(training_csv,index=False)\n",
    "\n",
    "original_df = df\n",
    "shuffled_df = original_df.sample(frac=1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "add97065",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "add97065",
    "outputId": "9fb3458e-3f95-432f-c15b-a42d05a5b516"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "      <th>file</th>\n",
       "      <th>relevance</th>\n",
       "      <th>Acknowledgement</th>\n",
       "      <th>Alternation</th>\n",
       "      <th>Background</th>\n",
       "      <th>Clarification_question</th>\n",
       "      <th>...</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>Correction</th>\n",
       "      <th>Elaboration</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Narration</th>\n",
       "      <th>Parallel</th>\n",
       "      <th>Q-Elab</th>\n",
       "      <th>Question-answer_pair</th>\n",
       "      <th>Result</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PM</td>\n",
       "      <td>Okay</td>\n",
       "      <td>0</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PM</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PM</td>\n",
       "      <td>&lt;vocalsound&gt; Um well this is the kick-off meet...</td>\n",
       "      <td>2</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.954745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PM</td>\n",
       "      <td>Um &lt;vocalsound&gt; and um</td>\n",
       "      <td>3</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.531381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PM</td>\n",
       "      <td>this is just what we're gonna be doing over th...</td>\n",
       "      <td>4</td>\n",
       "      <td>ES2002a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.949238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 speaker                                               text  \\\n",
       "0           0      PM                                               Okay   \n",
       "1           1      PM                                              Right   \n",
       "2           2      PM  <vocalsound> Um well this is the kick-off meet...   \n",
       "3           3      PM                             Um <vocalsound> and um   \n",
       "4           4      PM  this is just what we're gonna be doing over th...   \n",
       "\n",
       "   index     file  relevance  Acknowledgement  Alternation  Background  \\\n",
       "0      0  ES2002a          0                0            0           0   \n",
       "1      1  ES2002a          0                0            0           0   \n",
       "2      2  ES2002a          1                0            0           0   \n",
       "3      3  ES2002a          0                0            0           0   \n",
       "4      4  ES2002a          0                0            0           0   \n",
       "\n",
       "   Clarification_question  ...  Contrast  Correction  Elaboration  \\\n",
       "0                       0  ...         0           0            0   \n",
       "1                       0  ...         0           0            0   \n",
       "2                       0  ...         0           0            0   \n",
       "3                       0  ...         0           0            0   \n",
       "4                       0  ...         0           0            1   \n",
       "\n",
       "   Explanation  Narration  Parallel  Q-Elab  Question-answer_pair  Result  \\\n",
       "0            0          0         0       0                     0       0   \n",
       "1            0          0         0       0                     0       0   \n",
       "2            0          0         0       0                     0       0   \n",
       "3            1          0         0       0                     0       0   \n",
       "4            0          0         0       0                     0       0   \n",
       "\n",
       "      score  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  3.954745  \n",
       "3  3.531381  \n",
       "4  4.949238  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993ecbdd",
   "metadata": {
    "id": "993ecbdd"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43506e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training :  12524\n",
      "validation :  50099\n",
      "training + validation :  62623\n",
      "test :  10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testing_size = 10000\n",
    "split = 0.16 #iftraining bert, else 0.8 for just tfidf and classifier because 0.16 takes to much time to execute\n",
    "split_text = \"016\"\n",
    "\n",
    "train_df, val_df, test_df = split_dataset (shuffled_df, split,testing_size)\n",
    "\n",
    "print(\"training : \", train_df.shape[0])\n",
    "print(\"validation : \", val_df.shape[0])\n",
    "print(\"training + validation : \", train_df.shape[0]+val_df.shape[0])\n",
    "print(\"test : \", test_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce59279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cce59279",
    "outputId": "61034c35-c074-4c58-c0c4-70ff32acc47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit and transform done\n",
      "test transform done\n",
      "training samples :  10000\n",
      "embedding dimension :  317\n",
      "classifier trained\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.78      0.85     40941\n",
      "           1       0.43      0.74      0.54      9158\n",
      "\n",
      "    accuracy                           0.77     50099\n",
      "   macro avg       0.68      0.76      0.69     50099\n",
      "weighted avg       0.84      0.77      0.79     50099\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_val, predictions))\n\u001b[0;32m---> 47\u001b[0m mean_svc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m f1_score(y_val,predictions,pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#size = size*2\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_svc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_sample_weights(y, class_weights):\n",
    "    sample_weights = [class_weights[label] for label in y]\n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "\n",
    "train_df = train_df [0:10000]\n",
    "\n",
    "additional_features = tags + [\"score\"]\n",
    "max_embedding = 300\n",
    "\n",
    "class_weights = [1,3]\n",
    "df = shuffled_df\n",
    "x_train_sentences = train_df [\"text\"].values.tolist()\n",
    "y_train = train_df[\"relevance\"].values.tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_embedding)\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train_sentences)\n",
    "\n",
    "print(\"fit and transform done\")\n",
    "\n",
    "x_train_combined = np.concatenate([x_train_vectorized.toarray(), train_df[additional_features]], axis=1)\n",
    "print(\"test transform done\")\n",
    "print(\"training samples : \", len(y_train))\n",
    "print(\"embedding dimension : \", x_train_combined.shape[1])\n",
    "\n",
    "\n",
    "classifier = SVC(kernel='linear',class_weight={0:class_weights[0],1:class_weights[1]})\n",
    "classifier.fit(x_train_combined, y_train)\n",
    "\n",
    "print(\"classifier trained\")\n",
    "\n",
    "x_val_sentences = val_df [\"text\"].values.tolist()\n",
    "y_val = val_df[\"relevance\"].values.tolist()\n",
    "\n",
    "x_val_vectorized = vectorizer.transform(x_val_sentences)\n",
    "\n",
    "x_val_combined = np.concatenate([x_val_vectorized.toarray(), val_df[additional_features]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = classifier.predict(x_val_combined)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, predictions))\n",
    "#size = size*2\n",
    "\n",
    "print(\"ADA\")\n",
    "\n",
    "sample_weights = compute_sample_weights(y_train, class_weights)\n",
    "ada_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_classifier.fit(x_train_combine, y_val, sample_weight=sample_weights)\n",
    "\n",
    "ada_predictions = ada_classifier.predict(x_val_combined)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, ada_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c4750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "history_visible": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
